# Gemini API í†µí•© ê°€ì´ë“œ

## ì™œ Geminiì¸ê°€?

### Ollama llama3.2 vs Gemini ë¹„êµ

| í•­ëª© | Ollama llama3.2 | Gemini 1.5 Flash | Gemini 1.5 Pro |
|------|-----------------|------------------|----------------|
| **JSON Mode** | âŒ ì—†ìŒ | âœ… ì§€ì› | âœ… ì§€ì› |
| **íŒŒì‹± ì„±ê³µë¥ ** | ~20% | ~95% | ~98% |
| **ì†ë„** | ëŠë¦¼ (18ì´ˆ/ê·¸ë£¹) | ë¹ ë¦„ (1-2ì´ˆ/ê·¸ë£¹) | ì¤‘ê°„ (2-3ì´ˆ/ê·¸ë£¹) |
| **ë¹„ìš© (108ê·¸ë£¹)** | ë¬´ë£Œ | ~30ì› | ~500ì› |
| **í’ˆì§ˆ** | ë³´í†µ | ìš°ìˆ˜ | ë§¤ìš° ìš°ìˆ˜ |
| **ë©”ëª¨ë¦¬** | 2GB | 0MB (API) | 0MB (API) |

### Geminiì˜ í•µì‹¬ ì¥ì 

#### 1. JSON Mode (Native Support)
```python
generation_config = {
    "response_mime_type": "application/json",  # ìˆœìˆ˜ JSONë§Œ ë°˜í™˜
    "response_schema": {  # ìŠ¤í‚¤ë§ˆ ê°•ì œ
        "type": "object",
        "properties": {...}
    }
}
```

**íš¨ê³¼:**
- âœ… ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ **ì›ì²œ ì°¨ë‹¨**
- âœ… ì¶”ê°€ ì„¤ëª… í…ìŠ¤íŠ¸ **ë¶ˆê°€ëŠ¥**
- âœ… ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ì‹œ ìë™ ì¬ì‹œë„

#### 2. ë¹„ìš© íš¨ìœ¨ì„±
```
PETNER-backend ë¶„ì„ (229íŒŒì¼, 108ê·¸ë£¹) ê¸°ì¤€:

Gemini 1.5 Flash:
- Input: 216K tokens Ã— $0.075/1M = $0.016
- Output: 21.6K tokens Ã— $0.30/1M = $0.006
- ì´: $0.022 (ì•½ 30ì›)

Gemini 1.5 Pro:
- Input: 216K tokens Ã— $1.25/1M = $0.27
- Output: 21.6K tokens Ã— $5/1M = $0.11
- ì´: $0.378 (ì•½ 500ì›)

GPT-4 Turbo (ì°¸ê³ ):
- ì´: $2.81 (ì•½ 3,700ì›)
```

#### 3. ì†ë„
- **Gemini 1.5 Flash**: í‰ê·  1-2ì´ˆ/ê·¸ë£¹ â†’ 108ê·¸ë£¹ ì•½ **3-5ë¶„**
- **Gemini 1.5 Pro**: í‰ê·  2-3ì´ˆ/ê·¸ë£¹ â†’ 108ê·¸ë£¹ ì•½ **5-8ë¶„**
- **Ollama llama3.2**: í‰ê·  18ì´ˆ/ê·¸ë£¹ â†’ 108ê·¸ë£¹ ì•½ **30ë¶„+**

---

## êµ¬í˜„ ë°©ë²•

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜

`requirements.txt`ì— ì¶”ê°€:
```txt
# Google Gemini API
google-generativeai==0.3.2
```

ì„¤ì¹˜:
```bash
pip install google-generativeai==0.3.2
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼:
```env
# LLM Provider
LLM_PROVIDER=gemini

# Gemini Configuration
GEMINI_API_KEY=your-api-key-here
GEMINI_MODEL=gemini-1.5-flash  # ë˜ëŠ” gemini-1.5-pro

# Existing settings
GLOBAL_SEMAPHORE_LIMIT=2
INTERNAL_SEMAPHORE_LIMIT=10
TEMP_WORKSPACE=/tmp
HOST=0.0.0.0
PORT=8000
```

### 3. Config ì—…ë°ì´íŠ¸

`app/config.py`:
```python
from pydantic_settings import BaseSettings
from typing import Literal


class Settings(BaseSettings):
    """Application settings from environment variables"""

    # LLM Configuration
    llm_provider: Literal["openai", "ollama", "gemini"] = "gemini"

    # OpenAI
    openai_api_key: str = ""
    openai_model: str = "gpt-4-turbo-preview"

    # Ollama
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3.2"

    # Gemini (ì¶”ê°€)
    gemini_api_key: str = ""
    gemini_model: str = "gemini-1.5-flash"

    # ... ê¸°ì¡´ ì„¤ì • ìœ ì§€
```

### 4. AIService ì—…ë°ì´íŠ¸

`app/services/ai_service.py`:

```python
import asyncio
import logging
import json
from pathlib import Path
from typing import List, Dict, Any

from app.config import settings
from app.core.semaphore import semaphore_manager
from app.core.exceptions import AIServiceError
from app.utils.retry import ai_retry

logger = logging.getLogger(__name__)


class AIService:
    """AI-powered code analysis using Map-Reduce pattern"""

    def __init__(self):
        self.provider = settings.llm_provider

        if self.provider == "openai":
            from openai import AsyncOpenAI
            self.client = AsyncOpenAI(api_key=settings.openai_api_key)
            self.model = settings.openai_model

        elif self.provider == "ollama":
            import ollama
            self.client = ollama.AsyncClient(host=settings.ollama_base_url)
            self.model = settings.ollama_model

        elif self.provider == "gemini":
            import google.generativeai as genai
            genai.configure(api_key=settings.gemini_api_key)
            self.client = genai.GenerativeModel(
                model_name=settings.gemini_model,
                generation_config={
                    "response_mime_type": "application/json",  # JSON Mode!
                    "temperature": 0.3,
                    "max_output_tokens": 2000,
                }
            )
            self.model = settings.gemini_model

        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")

        logger.info(f"AIService initialized with provider: {self.provider}, model: {self.model}")

    # ... ê¸°ì¡´ map_analysis, reduce_analysis ë©”ì†Œë“œ ìœ ì§€ ...

    @ai_retry
    async def _call_ai_api(self, prompt: str, is_map: bool) -> Dict[str, Any]:
        """Call AI API with appropriate system prompt"""
        try:
            system_prompt = self._get_system_prompt(is_map)

            if self.provider == "openai":
                response = await self._call_openai(system_prompt, prompt)
            elif self.provider == "ollama":
                response = await self._call_ollama(system_prompt, prompt)
            elif self.provider == "gemini":
                response = await self._call_gemini(system_prompt, prompt)
            else:
                raise AIServiceError(f"Unsupported provider: {self.provider}")

            # Parse JSON response
            return self._parse_ai_response(response, is_map)

        except Exception as e:
            raise AIServiceError(f"AI API call failed: {e}")

    async def _call_gemini(self, system_prompt: str, user_prompt: str) -> str:
        """Call Gemini API"""
        try:
            # GeminiëŠ” system_promptë¥¼ user message ì•ì— ì¶”ê°€
            full_prompt = f"{system_prompt}\n\n{user_prompt}"

            # Async call (run_in_executor ì‚¬ìš©)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.generate_content(full_prompt)
            )

            return response.text

        except Exception as e:
            raise AIServiceError(f"Gemini API call failed: {e}")

    def _get_system_prompt(self, is_map: bool) -> str:
        """Get appropriate system prompt for Map or Reduce phase"""
        if is_map:
            # Map phase - íŒŒì¼ ê·¸ë£¹ ë¶„ì„
            if self.provider == "gemini":
                return """You are a Senior Code Reviewer.

Analyze the code files in XML format. Focus on the lines contributed by the target user.

Output ONLY a valid JSON object (no markdown, no extra text):
{
  "files_analyzed": ["file1.java", "file2.java"],
  "main_features": "ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª… (í•œê¸€, 1-2 ë¬¸ì¥)",
  "tech_stack": ["Java", "Spring", "Redis"],
  "notable_patterns": "ì£¼ëª©í•  ë§Œí•œ ì½”ë“œ íŒ¨í„´ (í•œê¸€, 1-2 ë¬¸ì¥)"
}"""
            else:
                # Ollama/OpenAI ê¸°ì¡´ í”„ë¡¬í”„íŠ¸
                return """You are a Senior Code Reviewer.

Analyze the provided code files in XML format. Focus on the lines contributed by the target user (indicated by <user_lines>).

Provide a concise summary in Korean JSON format:
{
  "files_analyzed": ["file1.java", "file2.java"],
  "main_features": "ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª… (í•œê¸€, 1-2 ë¬¸ì¥)",
  "tech_stack": ["Java", "Spring", "Redis"],
  "notable_patterns": "ì£¼ëª©í•  ë§Œí•œ ì½”ë“œ íŒ¨í„´ ë˜ëŠ” ì„¤ê³„ (í•œê¸€, 1-2 ë¬¸ì¥)"
}

Be concise and focus on what the code does, not how it's written."""

        else:  # Reduce phase
            if self.provider == "gemini":
                return """You are a Tech Lead preparing a portfolio report.

Given:
1. PROJECT STRUCTURE: Full directory tree
2. CODE ANALYSIS SUMMARIES: Individual analyses of code chunks

Output ONLY a valid JSON object (no markdown, no extra text):
{
  "summary": "ì´ ê°œë°œìì˜ ì „ì²´ ê¸°ì—¬ë„ì™€ ì£¼ìš” ì‘ì—… ë‚´ìš© (í•œê¸€, 3-5 ë¬¸ì¥)",
  "tech_stack": ["Java", "Spring Boot", "Redis", "WebSocket"],
  "key_contributions": [
    "ê¸°ì—¬ ë‚´ìš© 1 (êµ¬ì²´ì ìœ¼ë¡œ, í•œê¸€)",
    "ê¸°ì—¬ ë‚´ìš© 2",
    "ê¸°ì—¬ ë‚´ìš© 3"
  ],
  "code_quality": "ì½”ë“œ í’ˆì§ˆ í‰ê°€ (ì„¤ê³„ íŒ¨í„´, ì›ì¹™ ì¤€ìˆ˜ ë“±, í•œê¸€, 2-3 ë¬¸ì¥)",
  "project_tree": "COPY THE EXACT PROJECT STRUCTURE PROVIDED"
}"""
            else:
                # Ollama/OpenAI ê¸°ì¡´ í”„ë¡¬í”„íŠ¸
                return """You are a Tech Lead preparing a portfolio report.

You are given:
1. PROJECT STRUCTURE: Full directory tree of the project
2. CODE ANALYSIS SUMMARIES: Individual analyses of code chunks

Synthesize this information into a structured report in strict JSON format.

**IMPORTANT**: Your response MUST be valid JSON only. No markdown, no explanations, just JSON.

Output JSON schema:
{
  "summary": "ì´ ê°œë°œìì˜ ì „ì²´ ê¸°ì—¬ë„ì™€ ì£¼ìš” ì‘ì—… ë‚´ìš© (í•œê¸€, 3-5 ë¬¸ì¥)",
  "tech_stack": ["Java", "Spring Boot", "Redis", "WebSocket"],
  "key_contributions": [
    "ê¸°ì—¬ ë‚´ìš© 1 (êµ¬ì²´ì ìœ¼ë¡œ, í•œê¸€)",
    "ê¸°ì—¬ ë‚´ìš© 2",
    "ê¸°ì—¬ ë‚´ìš© 3"
  ],
  "code_quality": "ì½”ë“œ í’ˆì§ˆ í‰ê°€ (ì„¤ê³„ íŒ¨í„´, ì›ì¹™ ì¤€ìˆ˜ ë“±, í•œê¸€, 2-3 ë¬¸ì¥)",
  "project_tree": "COPY THE EXACT PROJECT STRUCTURE PROVIDED"
}

Return ONLY valid JSON. No markdown code blocks."""

    def _parse_ai_response(self, response: str, is_map: bool) -> Dict[str, Any]:
        """Parse AI response into structured data"""

        # Gemini JSON ModeëŠ” ì´ë¯¸ ìˆœìˆ˜ JSON ë°˜í™˜
        if self.provider == "gemini":
            try:
                return json.loads(response)
            except json.JSONDecodeError as e:
                logger.error(f"Gemini JSON parsing failed: {e}")
                logger.error(f"Response was: {response}")
                return self._get_fallback_response(is_map)

        # Ollama/OpenAIëŠ” ê¸°ì¡´ íŒŒì‹± ë¡œì§ ì‚¬ìš©
        try:
            # Remove markdown code blocks if present
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]

            cleaned = cleaned.strip()

            # Parse JSON
            return json.loads(cleaned)

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            logger.error(f"Response was: {response}")
            return self._get_fallback_response(is_map)

    def _get_fallback_response(self, is_map: bool) -> Dict[str, Any]:
        """Fallback response when parsing fails"""
        if is_map:
            return {
                "files_analyzed": [],
                "main_features": "ë¶„ì„ ì‹¤íŒ¨",
                "tech_stack": [],
                "notable_patterns": "ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"
            }
        else:
            return {
                "summary": "ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "tech_stack": [],
                "key_contributions": [],
                "code_quality": "í‰ê°€ ë¶ˆê°€",
                "project_tree": ""
            }
```

---

## í…ŒìŠ¤íŠ¸

### 1. API í‚¤ ë°œê¸‰

1. https://aistudio.google.com/app/apikey ì ‘ì†
2. "Create API Key" í´ë¦­
3. API í‚¤ ë³µì‚¬

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼:
```env
LLM_PROVIDER=gemini
GEMINI_API_KEY=your-actual-api-key-here
GEMINI_MODEL=gemini-1.5-flash
```

### 3. ì„œë²„ ì¬ì‹œì‘

```bash
# FastAPI ì¬ì‹œì‘
python -m app.main
```

ë¡œê·¸ í™•ì¸:
```
AIService initialized with provider: gemini, model: gemini-1.5-flash
```

### 4. í…ŒìŠ¤íŠ¸ ìš”ì²­

```bash
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "repo_url": "https://github.com/Dangdaengdan/PETNER-backend.git",
    "branch": "dev",
    "target_user": "LEE SUN MIN",
    "report_id": 5,
    "callback_url": "http://localhost:9000/api/reports/5"
  }'
```

**ì˜ˆìƒ ê²°ê³¼:**
- âœ… íŒŒì‹± ì—ëŸ¬ ê±°ì˜ ì—†ìŒ
- âœ… 108ê°œ ê·¸ë£¹ 3-5ë¶„ ë‚´ ì™„ë£Œ
- âœ… ê¹”ë”í•œ JSON ì‘ë‹µ

---

## ë¹„ìš© ìµœì í™”

### Flash vs Pro ì„ íƒ ê°€ì´ë“œ

**Gemini 1.5 Flash ì¶”ì²œ (ê¸°ë³¸):**
- âœ… ì†ë„ ë¹ ë¦„ (1-2ì´ˆ/ê·¸ë£¹)
- âœ… ë¹„ìš© ì €ë ´ (30ì›/108ê·¸ë£¹)
- âœ… í’ˆì§ˆ ì¶©ë¶„ (POC + í”„ë¡œë•ì…˜)

**Gemini 1.5 Pro ì¶”ì²œ:**
- ë” ì •í™•í•œ ë¶„ì„ í•„ìš” ì‹œ
- ë³µì¡í•œ ì½”ë“œ íŒ¨í„´ ë¶„ì„
- í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤

### ë¹„ìš© ê³„ì‚°ê¸°

```python
def estimate_cost(num_groups: int, model: str = "flash"):
    avg_input_tokens = 2000  # ê·¸ë£¹ë‹¹ í‰ê· 
    avg_output_tokens = 200

    if model == "flash":
        input_cost = 0.075 / 1_000_000
        output_cost = 0.30 / 1_000_000
    else:  # pro
        input_cost = 1.25 / 1_000_000
        output_cost = 5.00 / 1_000_000

    total_input = num_groups * avg_input_tokens * input_cost
    total_output = num_groups * avg_output_tokens * output_cost

    return total_input + total_output

# ì˜ˆì‹œ
print(f"Flash 108ê·¸ë£¹: ${estimate_cost(108, 'flash'):.3f}")  # ~$0.022
print(f"Pro 108ê·¸ë£¹: ${estimate_cost(108, 'pro'):.3f}")      # ~$0.378
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### API í‚¤ ì—ëŸ¬
```
Error: Invalid API key
```

**í•´ê²°:**
1. API í‚¤ í™•ì¸: https://aistudio.google.com/app/apikey
2. `.env` íŒŒì¼ ì¬í™•ì¸
3. ì„œë²„ ì¬ì‹œì‘

### Rate Limit
```
Error: Resource exhausted
```

**í•´ê²°:**
- Free tier: 15 RPM
- Internal Semaphoreë¥¼ 5ë¡œ ë‚®ì¶”ê¸°
- API ì—…ê·¸ë ˆì´ë“œ ê³ ë ¤

### JSON íŒŒì‹± ì‹¤íŒ¨ (ë“œë¬¾)
```
ERROR - Gemini JSON parsing failed
```

**ì›ì¸:** ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜

**í•´ê²°:**
- Retry ìë™ ì‹¤í–‰ë¨ (tenacity)
- Fallback ì‘ë‹µ ë°˜í™˜

---

## ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ (ì˜ˆìƒ)

| ëª¨ë¸ | 108ê·¸ë£¹ ì†Œìš”ì‹œê°„ | íŒŒì‹± ì„±ê³µë¥  | ë¹„ìš© |
|------|-----------------|-----------|------|
| Ollama llama3.2 | 30-60ë¶„ | 20% | ë¬´ë£Œ |
| **Gemini Flash** | **3-5ë¶„** | **95%** | **30ì›** |
| Gemini Pro | 5-8ë¶„ | 98% | 500ì› |
| GPT-4 Turbo | 5-8ë¶„ | 97% | 3,700ì› |

**ê²°ë¡ :** Gemini 1.5 Flashê°€ **ìµœê³ ì˜ ê°€ì„±ë¹„** ğŸ†

---

**ì‘ì„±ì¼**: 2026-01-08
**ì‘ì„±ì**: AI Code Analysis POC Team
